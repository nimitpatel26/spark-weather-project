{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1619471388794_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-75-85.ec2.internal:20888/proxy/application_1619471388794_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-64-108.ec2.internal:8042/node/containerlogs/container_1619471388794_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, asc, desc, to_timestamp, unix_timestamp, from_unixtime\n",
    "from pyspark.sql.functions import lit, lag, col\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LagGather:\n",
    "    def __init__(self):\n",
    "        self.nLags = 0\n",
    "        self.FeatureNames = []\n",
    "\n",
    "    def setLagLength(self, nLags):\n",
    "        self.nLags = nLags\n",
    "        return self\n",
    "\n",
    "    def setInputCol(self, colname):\n",
    "        self.columnName = colname\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.withColumn(\"Series\", lit('Univariate'))\n",
    "        mywindow = Window.orderBy(\"Series\")\n",
    "        for i in range(self.nLags):\n",
    "            strLag = self.columnName + '_LagBy_' + str(i + 1)\n",
    "            df = df.withColumn(strLag, lag(df[self.columnName], i + 1).over(mywindow))\n",
    "            self.FeatureNames.append(strLag)\n",
    "        df = df.drop(\"Series\")\n",
    "        return df\n",
    "\n",
    "    def getFeatureNames(self):\n",
    "        return self.FeatureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Forecast(df, forecast_hours, nLags, timeSeriesColumn, sparksession):\n",
    "    LeadWindow = Window.rowsBetween(0, forecast_hours)\n",
    "    df = df.withColumn(\"label\", func.last(df[timeSeriesColumn]).over(LeadWindow))\n",
    "    \n",
    "    features = [timeSeriesColumn]\n",
    "    \n",
    "    # Auto-regression feature\n",
    "    LagTransformer = LagGather().setLagLength(nLags).setInputCol(timeSeriesColumn)\n",
    "    df = LagTransformer.transform(df)\n",
    "    featuresGenerated = LagTransformer.getFeatureNames()\n",
    "    features.extend(featuresGenerated)\n",
    "    \n",
    "    \n",
    "    df = df.dropna()\n",
    "    vA = VectorAssembler().setInputCols(features).setOutputCol(\"features\")\n",
    "    df_m = vA.transform(df)\n",
    "    \n",
    "    \n",
    "    # Splitting data into train, test\n",
    "    splitratio = 0.7\n",
    "    df_train, df_test = TimeSeriesSplit(df_m, splitratio, sparksession)\n",
    "    \n",
    "    \n",
    "    rfr = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxDepth=5, subsamplingRate=0.8, )\n",
    "    model = rfr.fit(df_train)\n",
    "    predictions_rfr_test = model.transform(df_test)\n",
    "    predictions_rfr_train = model.transform(df_train)\n",
    "\n",
    "    # RMSE is used as evaluation metric\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "    RMSE_rfr_test = evaluator.evaluate(predictions_rfr_test)\n",
    "    RMSE_rfr_train = evaluator.evaluate(predictions_rfr_train)\n",
    "    return (df_test, df_train, predictions_rfr_test, predictions_rfr_train, RMSE_rfr_test, RMSE_rfr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TimeSeriesSplit(df_m, splitRatio, sparksession):\n",
    "    newSchema = StructType(df_m.schema.fields + [StructField(\"Row Number\", LongType(), False)])\n",
    "    new_rdd = df_m.rdd.zipWithIndex().map(lambda x: list(x[0]) + [x[1]])\n",
    "    df_m2 = sparksession.createDataFrame(new_rdd, newSchema)\n",
    "    total_rows = df_m2.count()\n",
    "    splitFraction = int(total_rows * splitRatio)\n",
    "    df_train = df_m2.where(df_m2[\"Row Number\"] >= 0).where(df_m2[\"Row Number\"] <= splitFraction)\n",
    "    df_test = df_m2.where(df_m2[\"Row Number\"] > splitFraction)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Difference(df, inputCol, outputCol):\n",
    "    lag1Window = Window.rowsBetween(-1, 0)\n",
    "    df = df.withColumn(outputCol, df[inputCol] - func.first(df[inputCol]).over(lag1Window))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Differenced Predictions Into Raw Predicions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Predict(i, df1, df2, timeSeriesCol, predictionCol, joinCol):\n",
    "    dZCol = 'DeltaZ' + str(i)\n",
    "    f_strCol = 'forecast_' + str(i) + 'hour'\n",
    "    df = df1.join(df2, [joinCol], how=\"inner\").orderBy(asc(\"dt_iso\"))\n",
    "    df = df.withColumnRenamed(predictionCol, dZCol)\n",
    "    df = df.withColumn(f_strCol, col(dZCol) + col(timeSeriesCol))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making Predictions And Saving To Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def SavePredictions(df, timeSeriesCol, forecast_hours, feature_nLags, filename, sparksession):\n",
    "    \n",
    "    diff_timeSeriesCol = \"Diff_\" + timeSeriesCol\n",
    "    df = Difference(df, timeSeriesCol, diff_timeSeriesCol)\n",
    "\n",
    "    RMSE_test = {}\n",
    "    RMSE_train = {}\n",
    "    corr_predict_train = {}\n",
    "    corr_predict_test = {}\n",
    "    strCol_prev = \"forecast_0hour\"\n",
    "\n",
    "    index = 0\n",
    "    for i in range(12, forecast_hours + 1, 12):\n",
    "\n",
    "        \n",
    "        df_test, df_train, predictions_test, predictions_train, RMSE_ts, RMSE_tr = Forecast(df.select(\"dt_iso\", timeSeriesCol, diff_timeSeriesCol), i, feature_nLags, diff_timeSeriesCol, sparksession)\n",
    "\n",
    "        RMSE_test.update({'forecast_' + str(i) + 'hour': RMSE_ts})\n",
    "        RMSE_train.update({'forecast_' + str(i) + 'hour': RMSE_tr})\n",
    "\n",
    "        \n",
    "        if (i == 12):\n",
    "            \n",
    "            corr_predict_train = Predict(i, df_train.select(\"Row Number\", \"dt_iso\", timeSeriesCol), predictions_train.select(\"Row Number\", \"prediction\"), timeSeriesCol, \"prediction\", \"Row Number\")\n",
    "            corr_predict_test = Predict(i, df_test.select(\"Row Number\", \"dt_iso\", timeSeriesCol), predictions_test.select(\"Row Number\", \"prediction\"), timeSeriesCol, \"prediction\", \"Row Number\")\n",
    "        else:\n",
    "            \n",
    "            strCol_prev = \"forecast_\" + str(i - 12) + \"hour\"\n",
    "            \n",
    "            corr_predict_train = Predict(i, corr_predict_train, predictions_train.select(\"Row Number\", \"prediction\"), strCol_prev, \"prediction\", \"Row Number\")\n",
    "            corr_predict_test = Predict(i, corr_predict_test, predictions_test.select(\"Row Number\", \"prediction\"), strCol_prev, \"prediction\", \"Row Number\")\n",
    "\n",
    "            LeadWindow = Window.rowsBetween(0, index)\n",
    "            a_strCol = \"actual_\" + str(i) + \"hour\"\n",
    "            corr_predict_test = corr_predict_test.withColumn(a_strCol, func.last(corr_predict_test[timeSeriesCol]).over(LeadWindow))\n",
    "            corr_predict_train = corr_predict_train.withColumn(a_strCol, func.last(corr_predict_test[timeSeriesCol]).over(LeadWindow))\n",
    "            \n",
    "            \n",
    "        index += 1\n",
    "    corr_predict_test.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(filename + \"test.csv\")\n",
    "    corr_predict_train.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(filename + \"train.csv\")\n",
    "\n",
    "\n",
    "    \n",
    "    return RMSE_train, RMSE_test, corr_predict_train, corr_predict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_data(fileLocation):\n",
    "    df = pd.read_csv(fileLocation)\n",
    "    del df [\"dt\"]\n",
    "    del df [\"timezone\"]\n",
    "    del df [\"city_name\"]\n",
    "    del df [\"lat\"]\n",
    "    del df [\"lon\"]\n",
    "    del df [\"sea_level\"]\n",
    "    del df [\"grnd_level\"]\n",
    "    del df [\"weather_icon\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting The Spark Application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************************************\n",
      "sf\n",
      "***************************************************\n",
      "{'forecast_12hour': 1.347097026810685, 'forecast_24hour': 1.0767214208591689, 'forecast_36hour': 1.3680354611696, 'forecast_48hour': 1.1389964586382983, 'forecast_60hour': 1.3765635853880558, 'forecast_72hour': 1.1634881704289837}\n",
      "{'forecast_12hour': 1.2684462018868932, 'forecast_24hour': 0.9691871290424079, 'forecast_36hour': 1.2864138145664687, 'forecast_48hour': 1.0278541305351683, 'forecast_60hour': 1.2928025946466204, 'forecast_72hour': 1.043000292304716}\n",
      "\n",
      "***************************************************\n",
      "seattle\n",
      "***************************************************\n",
      "{'forecast_12hour': 1.598093178660386, 'forecast_24hour': 1.5341791876506672, 'forecast_36hour': 1.6465735861809274, 'forecast_48hour': 1.6046341972766143, 'forecast_60hour': 1.6801126599995369, 'forecast_72hour': 1.650786169238132}\n",
      "{'forecast_12hour': 1.2478753022106495, 'forecast_24hour': 1.1707074761560423, 'forecast_36hour': 1.289352913604458, 'forecast_48hour': 1.2340726866157232, 'forecast_60hour': 1.3129717388221966, 'forecast_72hour': 1.269230664897236}"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.appName(\"TimeSeries\").master(\"local\").config(conf=conf).getOrCreate()\n",
    "\n",
    "data = []\n",
    "\n",
    "for city in [\"sf\", \"seattle\"]:\n",
    "    \n",
    "    print(\"\\n***************************************************\")\n",
    "    print(city)\n",
    "    print(\"***************************************************\")\n",
    "\n",
    "    df = get_data(\"s3://spark-weather-project/data/\" + city + \".csv\")\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "\n",
    "    timeSeriesCol = \"temp\"\n",
    "    forecast_hours = 72\n",
    "    num_lags = 3\n",
    "\n",
    "\n",
    "    RMSE_train, RMSE_test, train, test = SavePredictions(spark_df, timeSeriesCol, forecast_hours, num_lags, \"s3://spark-weather-project/data/\" + city + \"_\", spark)\n",
    "    \n",
    "    data.append([RMSE_train, RMSE_test, train, test])\n",
    "    \n",
    "    print(RMSE_train)\n",
    "    print(RMSE_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
